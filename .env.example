# Rename this file to .env once you have filled in the below environment variables!
#
# ===== 2025 LLM PROVIDERS UPDATE =====
# Updated with the latest models for code generation including:
# - OpenAI: o3-pro, o4-mini, GPT-5, GPT-4.1
# - Anthropic: Claude Opus 4, Sonnet 4 (world's best coding models)
# - Google: Gemini 2.5 series with 1M+ context and thinking
# - xAI: Grok 4, Code Fast 1 (70.8% SWE-bench performance)
# - DeepSeek: V3.1-Terminus, Coder-V2 (338 languages)
# - Mistral: Codestral 25.01 (#1 on LMsys copilot arena)
# - Plus additional providers: Cohere Command A, Hyperbolic, Together AI

# Get your GROQ API Key here -
# https://console.groq.com/keys
# You only need this environment variable set if you want to use Groq models
GROQ_API_KEY=

# Get your HuggingFace API Key here -
# https://huggingface.co/settings/tokens
# You only need this environment variable set if you want to use HuggingFace models
HuggingFace_API_KEY=


# Get your Open AI API Key by following these instructions -
# https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key
# Supports 2025 models: o3-pro, o3, o4-mini, o3-mini, GPT-5, GPT-4.1, GPT-4o series
OPENAI_API_KEY=

# Get your Anthropic API Key in your account settings -
# https://console.anthropic.com/settings/keys
# Supports 2025 models: Claude Opus 4, Sonnet 4, 3.7 Sonnet, 3.5 Sonnet, 3.5 Haiku
ANTHROPIC_API_KEY=

# Get your OpenRouter API Key in your account settings -
# https://openrouter.ai/settings/keys
# You only need this environment variable set if you want to use OpenRouter models
OPEN_ROUTER_API_KEY=

# Get your Google Generative AI API Key by following these instructions -
# https://console.cloud.google.com/apis/credentials
# Supports 2025 models: Gemini 2.5 Pro, 2.5 Flash, 2.0 Flash series with 1M+ context
GOOGLE_GENERATIVE_AI_API_KEY=

# You only need this environment variable set if you want to use oLLAMA models
# DONT USE http://localhost:11434 due to IPV6 issues
# USE EXAMPLE http://127.0.0.1:11434
OLLAMA_API_BASE_URL=

# You only need this environment variable set if you want to use OpenAI Like models
OPENAI_LIKE_API_BASE_URL=

# Get your Together AI API Key at https://api.together.xyz/settings/api-keys
# Supports 2025 models: Llama 4 Scout/Maverick, Llama 3.3, Mixtral models (200+ total)
TOGETHER_API_BASE_URL=
TOGETHER_API_KEY=

# Get your DeepSeek API Key at https://platform.deepseek.com/api_keys
# Supports 2025 models: DeepSeek-V3.1-Terminus (chat/reasoner), DeepSeek-Coder-V2
DEEPSEEK_API_KEY=

# Get your Moonshot (Kimi) API Key at https://platform.moonshot.ai/console/api-keys
# Supports 2025 models: Moonshot v1 series, Kimi latest/K2 thinking models (OpenAI-compatible)
MOONSHOT_API_KEY=

# Get your OpenAI Like API Key for custom OpenAI-compatible endpoints
OPENAI_LIKE_API_KEY=

# Get your Cerebras API Key at https://cloud.cerebras.ai/
# Ultra-fast LLaMA inference with Wafer-Scale Engine technology
CEREBRAS_API_KEY=

# Get your Hyperbolic API Key at https://app.hyperbolic.xyz/settings
# Cost-effective LLaMA and Mixtral models with OpenAI-compatible API
# baseURL="https://api.hyperbolic.xyz/v1/chat/completions"
HYPERBOLIC_API_KEY=
HYPERBOLIC_API_BASE_URL= 

# Get your Mistral API Key by following these instructions -
# https://console.mistral.ai/api-keys/
# Supports 2025 models: Codestral 25.01 (#1 LMsys copilot arena), Mistral Code
MISTRAL_API_KEY=

# Get the Cohere Api key by following these instructions -
# https://dashboard.cohere.com/api-keys
# Supports 2025 models: Command A 03-2025 (111B params, best throughput)
COHERE_API_KEY=

# Get LMStudio Base URL from LM Studio Developer Console
# Make sure to enable CORS
# DONT USE http://localhost:1234 due to IPV6 issues
# Example: http://127.0.0.1:1234
LMSTUDIO_API_BASE_URL=

# Get your xAI API key
# https://x.ai/api
# Supports 2025 models: Grok 4, Grok Code Fast 1 (70.8% SWE-bench), Grok 3 (1M context)
XAI_API_KEY=

# Get your Perplexity API Key here - 
# https://www.perplexity.ai/settings/api
# You only need this environment variable set if you want to use Perplexity models
PERPLEXITY_API_KEY=

# Get your AWS Bedrock configuration
# https://console.aws.amazon.com/iam/home
# Supports 2025 models: Claude Opus 4, Sonnet 4, Nova Pro, intelligent model routing
# The JSON should include the following keys:
#   - region: The AWS region where Bedrock is available.
#   - accessKeyId: Your AWS access key ID.
#   - secretAccessKey: Your AWS secret access key.
#   - sessionToken (optional): Temporary session token if using an IAM role or temporary credentials.
# Example JSON:
# {"region": "us-east-1", "accessKeyId": "yourAccessKeyId", "secretAccessKey": "yourSecretAccessKey", "sessionToken": "yourSessionToken"}
AWS_BEDROCK_CONFIG=

# Include this environment variable if you want more logging for debugging locally
VITE_LOG_LEVEL=debug

# Get your GitHub Personal Access Token here -
# https://github.com/settings/tokens
# This token is used for:
# 1. Importing/cloning GitHub repositories without rate limiting
# 2. Accessing private repositories
# 3. Automatic GitHub authentication (no need to manually connect in the UI)
# 4. GitHub Models API access (curated selection of top models)
#
# For classic tokens, ensure it has these scopes: repo, read:org, read:user
# For fine-grained tokens, ensure it has Repository and Organization access
VITE_GITHUB_ACCESS_TOKEN=

# GitHub Models API Key (same as GitHub token for most cases)
# Access to curated models: GPT-4o, o1, Claude 3.5, LLaMA, Gemini, Mistral, DeepSeek
GITHUB_API_KEY=

# Specify the type of GitHub token you're using
# Can be 'classic' or 'fine-grained'
# Classic tokens are recommended for broader access
VITE_GITHUB_TOKEN_TYPE=classic

# Example Context Values for qwen2.5-coder:32b
# 
# DEFAULT_NUM_CTX=32768 # Consumes 36GB of VRAM
# DEFAULT_NUM_CTX=24576 # Consumes 32GB of VRAM
# DEFAULT_NUM_CTX=12288 # Consumes 26GB of VRAM
# DEFAULT_NUM_CTX=6144 # Consumes 24GB of VRAM
DEFAULT_NUM_CTX=
